![Screenshot](ProjectScreenshots/Screenshots.jpg)

# Arabic Named Entity Recognition (ANER)

This project fine-tunes a pre-trained BERT model (`asafaya/bert-base-arabic`) for Named Entity Recognition (NER) on Arabic text using the WikiFANE_Gold_2014_500K dataset. It includes preprocessing, model training, evaluation, and a GUI demo using Gradio and Tkinter.

---

## ğŸ“Œ Project Overview

- **Goal:** Automatically identify and classify named entities (e.g., people, places, organizations) in Arabic text.
- **Challenges Addressed:**
  - Arabic diacritics and script normalization
  - Morphological complexity
  - Entity boundary ambiguity

---

## ğŸ“‚ Dataset & Preprocessing

- **Dataset Used:** [WikiFANE_Gold_2014_500K](https://fsalotaibi.kau.edu.sa/Pages-Arabic-NE-Corpora.aspx)
- **Stats:**
  - ~500K tokens, 15,763 sentences
  - 102 unique entity tags (e.g., Nation, Politician, Population-Center)
- **Preprocessing Steps:**
  - Normalization (remove diacritics, unify characters)
  - Token masking for data augmentation
  - Train/test split: 90% / 10%

---

## ğŸ§  Model Architecture

- **Base Model:** `asafaya/bert-base-arabic` (110M+ parameters)
- **Head:** Token classification layer
- **Dropout:** 0.3
- **Label Alignment:** For subword tokens

---

## ğŸ‹ï¸ Training Configuration

- **Learning Rate:** 2e-5  
- **Batch Size:** 8  
- **Epochs:** 3 (early stopping with patience = 2)  
- **Hardware:** Google Colab GPU (T4)

---

## ğŸ“Š Results

| Metric              | Value       |
|---------------------|-------------|
| Token Accuracy      | ~95%      |
| F1-Score (micro)    | ~0.68       |

### Entity-Level Performance

- **Nation:** F1 = 0.81  
- **Population-Center:** F1 = 0.66  
- **Politician:** F1 = 0.64

---

## ğŸš€ Demo & Inference

- **Input:**  
  `"ÙˆÙÙ„ÙØ¯Ù Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù† ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©."`

- **Output:**
  - Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù† â†’ Politician (96%)
  - Ø§Ù„Ø±ÙŠØ§Ø¶ â†’ Population-Center (98%)
  - Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© â†’ Nation (94%)

- **Tools:**
  - `Gradio` for web-based testing
  - `Tkinter` for local desktop GUI

---

## âš ï¸ Limitations

1. **Sequence Length:** Truncated context due to BERTâ€™s 128-token limit.
2. **Arabic-only Model:** Because we fine-tune on arabic dataset.
3. **Overfitting:** Slight generalization drop after Epoch 1 (So we take model from epoch 1).
4. **Context Window:** Long sentences may lose coherence.
5. **Latency & Model Size:** Large models can be slow or memory-intensive.

  **Potential Fixes:**
- Knowledge Distillation  
- Model Quantization

---

## âœ… Conclusion

- High-accuracy Arabic NER achieved through BERT fine-tuning.
- Live demo via Gradio & Tkinter.
- Open-source for research and practical applications.
